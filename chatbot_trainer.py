# -*- coding: utf-8 -*-
"""Chatbot_Trainer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/179j_3P6784AZZzwIwgfsGHo1xRgBsSAe
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x

import sys
print("Python {}".format(sys.version))

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Dense, Activation
print("Tensorflow {}".format(tf.__version__))

import numpy as np
print("Numpy {}".format(np.__version__))

import nltk
nltk.download("punkt")
from nltk.stem.lancaster import LancasterStemmer
print("Natural Language Toolkit {}".format(nltk.__version__))


import random
import json

# Meta paramaeters
num_epochs = 500
batch_size = 8
num_neurons_hl = 8
activation_function = "relu"

excluded_characters = ('?')

model_name = "chatbot_model_test.h5"

intents_path = "intents.json"
stemmer = LancasterStemmer()

with open(intents_path) as file:
  data = json.load(file)

words = []
labels = []
docs_x = []
docs_y = []

for intent in data["intents"]:
  for pattern in intent["patterns"]:
    # For this we need to tokenize the words there.
    wrds = nltk.word_tokenize(pattern) # This returns a list of individual words.

    words.extend(wrds)
    docs_x.append(wrds)
    docs_y.append(intent["tag"]) # Keep track of the intent of each pattern.


    # Add tags that were not seen before
    if intent["tag"] not in labels:
      labels.append(intent["tag"])

# Stem each word using the natural a language kit stemmer.
# Exclude the question mark.
words = [stemmer.stem(w.lower()) for w in words if w not in excluded_characters]

# Sort the words
words = sorted(list(set(words)))

# Sort the labels aswell.
labels = sorted(labels)

# Create training inputs and outputs.
# Create a bag of words, one-hot encoding that turns words into tensors.
# Make a list where the position indicates the word, and the number on that index
# is either a 0 ar a 1 to tell if the word exists in the sentece.

training = []
output = []

out_empty = [0 for _ in range(len(labels))]

for x, doc in enumerate(docs_x):
  bag = []

  # Stemming the patterns: bring down is word to the root word.
  wrds = [stemmer.stem(w) for w in doc]

  # Iterate through all stemmed words of this sentence to compare with main list.
  for w in words:
    if w in wrds:
      bag.append(1)
    else:
      bag.append(0)

  # Generate the output and append to training and output.
  output_row = out_empty[:]
  output_row[labels.index(docs_y[x])] = 1

  training.append(bag)
  output.append(output_row)

# Convert python lists into numpy arrays to use as tensors.
training = np.array(training)
output = np.array(output)


def create_model():
  
  num_inputs =  len(training[0])
  num_outputs = len(output[0])
  
  # Make model with keras.
  model = Sequential()

  
  # Hidden layers
  model.add(Dense(units=num_neurons_hl, input_dim= num_inputs))
  model.add(Activation(activation_function))
  model.add(Dense(units=num_neurons_hl))
  model.add(Activation(activation_function))

  # Output layer
  model.add(Dense( num_outputs, activation="softmax"))

  model.compile(loss = "categorical_crossentropy", optimizer = "adam", metrics=["accuracy"])
  return model

model = create_model()
model.summary()

# Train with data.
model.fit(training, output, epochs=num_epochs, batch_size=batch_size)

# Save the model.
model.save(str("saved_model/" + model_name))

!mkdir -p saved_model

from google.colab import files
files.download("/content/saved_model/chatbot_model_test.h5")